# utils.py - Sminex AutoML v0.25 ULTIMATE - –í–°–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø
# ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
# - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ chardet –ø–æ hash —Ñ–∞–π–ª–∞ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ)
# - –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
# - Stratified sampling –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
# - –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
# - –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤

# ‚úÖ –°–†–ï–î–ù–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
# - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è memory usage
# - –õ—É—á—à–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–æ–∫
# - –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

import base64
import time
import datetime as _dt
import uuid
import streamlit as st
import pandas as pd
import numpy as np
from typing import Tuple, List, Optional, Dict
import os
import logging
import hashlib

logger = logging.getLogger(__name__)

# ============ –ü–û–ü–´–¢–ö–ê –ò–ú–ü–û–†–¢–ê CHARDET –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò –ö–û–î–ò–†–û–í–ö–ò ============
try:
    import chardet
    CHARDET_AVAILABLE = True
except ImportError:
    CHARDET_AVAILABLE = False
    logger.warning("chardet –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è UTF-8 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")

# ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ chardet
_ENCODING_CACHE: Dict[str, str] = {}

# ============ –ü–†–û–ì–†–ï–°–° –ò –í–†–ï–ú–Ø ============
def _sleep_with_progress(seconds: float, label: str = "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ..."):
    """–ü–ª–∞–≤–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä."""
    if seconds <= 0:
        return
    steps = min(30, int(seconds * 10))
    step = max(seconds / steps, 0.02)
    prog = st.progress(0)
    ph = st.empty()
    ph.info(label)
    for i in range(steps):
        time.sleep(step)
        prog.progress(min(100, int((i + 1) / steps * 100)))
    ph.empty()
    prog.empty()

def enforce_min_duration(start_time: float, min_seconds: float = 6.0, label: str = "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è..."):
    """–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–∏."""
    elapsed = time.time() - start_time
    remaining = max(0.0, min_seconds - elapsed)
    _sleep_with_progress(remaining, label)

def human_time_ms(ms: float) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å ms –≤ —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç."""
    if ms < 1_000:
        return f"{int(ms)} ms"
    s = ms / 1000
    if s < 60:
        return f"{s:.1f} s"
    m = int(s // 60)
    s = s - m * 60
    return f"{m}m {int(s)}s"


def format_eta(start_time: float, total_steps: int, completed_steps: int) -> str:
    """–û—Ü–µ–Ω–∫–∞ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""
    if completed_steps <= 0 or total_steps <= 0:
        return ""
    elapsed = time.time() - start_time
    avg = elapsed / completed_steps
    remaining = max(0.0, avg * (total_steps - completed_steps))
    if remaining < 60:
        return f"‚âà{int(remaining)}s"
    minutes = int(remaining // 60)
    seconds = int(remaining - minutes * 60)
    return f"‚âà{minutes}m {seconds:02d}s"

# ============ –°–ï–°–°–ò–Ø ============
def get_session_id() -> str:
    """–£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID —Å–µ—Å—Å–∏–∏."""
    if "_session_id" not in st.session_state:
        st.session_state["_session_id"] = str(uuid.uuid4())
    return st.session_state["_session_id"]

def get_ttl_to_4am() -> int:
    """TTL –¥–æ 4 —É—Ç—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–Ω—è."""
    now = _dt.datetime.now()
    cutoff = now.replace(hour=4, minute=0, second=0, microsecond=0)
    # –ï—Å–ª–∏ –≤—Ä–µ–º—è >= 4:00 AM, —Å—á–∏—Ç–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å
    if now >= cutoff:
        cutoff = cutoff + _dt.timedelta(days=1)
    ttl_seconds = max(60, int((cutoff - now).total_seconds()))
    return ttl_seconds

# ============ –°–ö–ê–ß–ò–í–ê–ù–ò–ï ============
def download_button(bytes_data: bytes, filename: str, label: str, mime: str = "application/octet-stream"):
    """–ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è."""
    b64 = base64.b64encode(bytes_data).decode("utf-8")
    href = f'<a download="{filename}" href="data:{mime};base64,{b64}" style="text-decoration:none;"><button style="padding:8px 16px;background:#007aff;color:white;border:none;border-radius:6px;cursor:pointer;">{label}</button></a>'
    st.markdown(href, unsafe_allow_html=True)

# ============ ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–ï–¢–ï–ö–¢–ò–†–û–í–ê–ù–ò–ï –ö–û–î–ò–†–û–í–ö–ò –° –ö–≠–®–ò–†–û–í–ê–ù–ò–ï–ú ============
def detect_file_encoding(file_bytes: bytes, cache_key: Optional[str] = None) -> str:
    """
    ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É —Ñ–∞–π–ª–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—ç—à –ø–æ hash —Ñ–∞–π–ª–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    - Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
    - –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    """
    # ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if cache_key and cache_key in _ENCODING_CACHE:
        logger.info(f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ: {_ENCODING_CACHE[cache_key]}")
        return _ENCODING_CACHE[cache_key]
    
    encoding = 'utf-8'  # default
    
    if CHARDET_AVAILABLE:
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 50KB –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            sample = file_bytes[:50000]
            detected = chardet.detect(sample)
            if detected and detected.get('encoding'):
                encoding = detected['encoding']
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω –∫–æ–¥–∏—Ä–æ–≤–æ–∫
                if encoding.lower() in ('ascii', 'utf-8-sig'):
                    encoding = 'utf-8'
                logger.info(f"chardet –æ–±–Ω–∞—Ä—É–∂–∏–ª –∫–æ–¥–∏—Ä–æ–≤–∫—É: {encoding} (confidence: {detected.get('confidence', 0):.2f})")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ chardet: {e}")
    
    if not CHARDET_AVAILABLE or encoding == 'utf-8':
        # Fallback: –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        for enc in ['utf-8', 'cp1251', 'iso-8859-1', 'latin-1', 'ascii']:
            try:
                file_bytes[:10000].decode(enc)
                encoding = enc
                logger.info(f"Fallback: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ {enc}")
                break
            except (UnicodeDecodeError, AttributeError):
                continue
    
    # ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    if cache_key:
        _ENCODING_CACHE[cache_key] = encoding
    
    return encoding

def get_file_hash(file_bytes: bytes) -> str:
    """‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –ü–æ–ª—É—á–∏—Ç—å hash —Ñ–∞–π–ª–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è."""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 1MB –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    sample = file_bytes[:1024*1024]
    return hashlib.md5(sample).hexdigest()

# ============ –î–ï–¢–ï–ö–¢–ò–†–û–í–ê–ù–ò–ï –†–ê–ó–î–ï–õ–ò–¢–ï–õ–Ø ============
def detect_csv_sep(first_bytes: bytes, encoding: str = 'utf-8') -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å CSV —Å —É—á–µ—Ç–æ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏.
    """
    try:
        text = first_bytes.decode(encoding, errors='ignore')
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è CSV: {e}")
        text = first_bytes.decode('utf-8', errors='ignore')
    
    candidates = [',', ';', '\t', '|']
    counts = {c: text.count(c) for c in candidates}
    
    if not counts or max(counts.values()) == 0:
        return ','
    
    best_sep = max(counts, key=counts.get)
    lines = text.split('\n')[:10]
    
    try:
        field_counts = [len(line.split(best_sep)) for line in lines if line.strip()]
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        if len(set(field_counts)) == 1 and field_counts[0] > 1:
            return best_sep
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ sep –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")
    
    return ','

# ============ ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: SMART SAMPLE –° STRATIFIED SAMPLING ============
def smart_sample_large_file(
    uploaded_file, 
    sep: str, 
    max_rows: int = 100000, 
    sample_size: int = 50000,
    encoding: str = 'utf-8',
    target_col: Optional[str] = None,
    task_type: Optional[str] = None
) -> pd.DataFrame:
    """
    ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ó–∞–≥—Ä—É–∑–∏—Ç—å –±–æ–ª—å—à–æ–π CSV —Å —É–º–Ω—ã–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    - –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ file pointer
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–æ–∫
    - –ù–∞–¥–µ–∂–Ω—ã–π parsing
    - ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: Stratified sampling –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    """
    try:
        uploaded_file.seek(0)
        
        # 1. –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞: —á–∏—Ç–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        chunk_size = 50000
        rows_estimate = 0
        chunks_to_est = 3
        
        try:
            temp_iter = pd.read_csv(
                uploaded_file, 
                sep=sep, 
                chunksize=chunk_size,
                encoding=encoding,
                on_bad_lines='skip',
                engine='python'
            )
            for i, chunk in enumerate(temp_iter):
                rows_estimate += len(chunk)
                if i >= chunks_to_est:
                    break
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–º–µ—Ä–∞: {e}")
            rows_estimate = 1000
        
        # –°–±—Ä–æ—Å–∏—Ç—å file pointer
        uploaded_file.seek(0)
        
        estimated_total = rows_estimate * (10 if rows_estimate > 0 else 1)
        
        # –ï—Å–ª–∏ –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª - —á–∏—Ç–∞–µ–º —Ü–µ–ª–∏–∫–æ–º
        if rows_estimate < max_rows and chunks_to_est < 3:
            uploaded_file.seek(0)
            df = pd.read_csv(
                uploaded_file, 
                sep=sep,
                encoding=encoding,
                on_bad_lines='skip',
                engine='python'
            )
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª: {df.shape}")
            return df
        
        # 2. –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        uploaded_file.seek(0)
        final_chunks = []
        total_collected = 0
        
        frac = min(1.0, (sample_size * 1.5) / max(1, estimated_total))
        
        try:
            reader = pd.read_csv(
                uploaded_file, 
                sep=sep, 
                chunksize=chunk_size,
                encoding=encoding,
                on_bad_lines='skip',
                engine='python'
            )
            for chunk in reader:
                if len(chunk) > 0:
                    sampled_chunk = chunk.sample(frac=frac, random_state=42)
                    final_chunks.append(sampled_chunk)
                    total_collected += len(sampled_chunk)
                    
                    if total_collected > max_rows * 1.2:
                        break
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
            uploaded_file.seek(0)
            return pd.read_csv(
                uploaded_file, 
                sep=sep,
                encoding=encoding,
                on_bad_lines='skip',
                nrows=sample_size,
                engine='python'
            )
        
        if not final_chunks:
            return pd.DataFrame()
        
        df = pd.concat(final_chunks, ignore_index=True)
        
        # ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Stratified sampling –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if len(df) > sample_size and target_col and target_col in df.columns and task_type in ('binary', 'multiclass'):
            try:
                from sklearn.model_selection import train_test_split
                # –£–¥–∞–ª—è–µ–º NaN –∏–∑ target
                df_valid = df[df[target_col].notna()].copy()
                
                if len(df_valid) > sample_size:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –∫–ª–∞—Å—Å—ã –∏–º–µ—é—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤
                    class_counts = df_valid[target_col].value_counts()
                    min_class_count = class_counts.min()
                    
                    if min_class_count >= 2:
                        # Stratified sampling
                        _, df_sampled = train_test_split(
                            df_valid,
                            test_size=sample_size / len(df_valid),
                            stratify=df_valid[target_col],
                            random_state=42
                        )
                        logger.info(f"–ü—Ä–∏–º–µ–Ω–µ–Ω stratified sampling: {len(df_sampled)} —Å—Ç—Ä–æ–∫")
                        return df_sampled.reset_index(drop=True)
                    else:
                        logger.warning(f"Stratified sampling –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω (–º–∏–Ω. –∫–ª–∞—Å—Å: {min_class_count})")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ stratified sampling: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º random")
        
        # –û–±—ã—á–Ω—ã–π random sampling
        if len(df) > sample_size:
            df = df.sample(n=sample_size, random_state=42)
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –≤—ã–±–æ—Ä–∫–∞ –∏–∑ –±–æ–ª—å—à–æ–≥–æ —Ñ–∞–π–ª–∞: {df.shape}")
        return df.reset_index(drop=True)
    
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
        raise

# ============ ‚úÖ –£–õ–£–ß–®–ï–ù–û: –í–ê–õ–ò–î–ê–¶–ò–Ø ============
def validate_data_types(train_df: pd.DataFrame, new_df: pd.DataFrame) -> Tuple[bool, List[str]]:
    """
    ‚úÖ –£–õ–£–ß–®–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π.
    - –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
    - –õ—É—á—à–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö
    - –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–∏–ø–æ–≤
    """
    errors = []
    warnings_list = []
    
    missing_cols = set(train_df.columns) - set(new_df.columns)
    if missing_cols:
        errors.append(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã: {list(missing_cols)}")
        return False, errors
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ inf –∏ NaN
    new_df_clean = new_df.replace([np.inf, -np.inf], np.nan)
    has_missing = new_df_clean.isnull().any().any()
    if has_missing:
        missing_pct = new_df_clean.isnull().sum().sum() / (new_df_clean.shape[0] * new_df_clean.shape[1]) * 100
        warnings_list.append(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∏ –∏–ª–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ({missing_pct:.1f}%)")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–∏–ø–æ–≤
    for col in train_df.columns:
        if col not in new_df.columns:
            continue
            
        train_type = train_df[col].dtype
        new_type = new_df[col].dtype
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏—è —á–∏—Å–ª–æ–≤—ã—Ö —Ç–∏–ø–æ–≤
        train_is_numeric = pd.api.types.is_numeric_dtype(train_type)
        new_is_numeric = pd.api.types.is_numeric_dtype(new_type)
        
        if train_is_numeric != new_is_numeric:
            try:
                if train_is_numeric:
                    new_df[col] = pd.to_numeric(new_df[col], errors='coerce')
                else:
                    new_df[col] = new_df[col].astype(str)
                warnings_list.append(f"‚ö†Ô∏è –°—Ç–æ–ª–±–µ—Ü '{col}' –ø–µ—Ä–µ–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {new_type} ‚Üí {train_type}")
            except Exception as e:
                errors.append(f"‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å '{col}': {str(e)[:50]}")
    
    all_messages = errors + warnings_list
    return len(errors) == 0, all_messages

def detect_positive_class(y: pd.Series, y_prob: np.ndarray) -> int:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
    if len(np.unique(y)) != 2 or y_prob.ndim != 2 or y_prob.shape[1] != 2:
        return 1
    
    class_means = np.mean(y_prob, axis=0)
    return int(np.argmax(class_means))

# ============ ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –ü–†–û–í–ï–†–ö–ê –ù–ê –î–£–ë–õ–ò–ö–ê–¢–´ –°–¢–†–û–ö ============
def check_and_remove_duplicates(df: pd.DataFrame, warn: bool = True) -> Tuple[pd.DataFrame, int]:
    """
    ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Å—Ç—Ä–æ–∫.
    
    Args:
        df: DataFrame –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        warn: –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    Returns:
        (–æ—á–∏—â–µ–Ω–Ω—ã–π DataFrame, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
    """
    original_len = len(df)
    df_clean = df.drop_duplicates()
    n_duplicates = original_len - len(df_clean)
    
    if n_duplicates > 0 and warn:
        dup_pct = (n_duplicates / original_len) * 100
        logger.warning(f"–ù–∞–π–¥–µ–Ω–æ {n_duplicates} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Å—Ç—Ä–æ–∫ ({dup_pct:.1f}%)")
        if dup_pct > 5:
            logger.warning("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ë–æ–ª–µ–µ 5% –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Å—Ç—Ä–æ–∫! "
                          "–≠—Ç–æ –º–æ–∂–µ—Ç –∏—Å–∫–∞–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏.")
    
    return df_clean, n_duplicates

# ============ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò ============
def get_random_tip() -> str:
    """–°–ª—É—á–∞–π–Ω—ã–π —Å–æ–≤–µ—Ç."""
    tips = [
        "üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–∏–Ω–∏–º—É–º 100 —Å—Ç—Ä–æ–∫ –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
        "üîÑ –ü—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ - –Ω–µ –≤—Å–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã",
        "üìä 80/20 - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π split train/test",
        "‚öôÔ∏è OPTUNA –Ω–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã",
        "üéØ –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
        "üìà ROC-AUC –ª—É—á—à–µ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
        "üîÆ What-If –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ",
        "üßπ –£–¥–∞–ª—è–π—Ç–µ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Å—Ç–æ–ª–±—Ü—ã –ò —Å—Ç—Ä–æ–∫–∏",
        "üé™ –ö–∞—Ç. –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ –ø–æ—Å–ª–µ OHE",
        "‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ —Ä–∞–∑—ã",
        "üé≤ Stratified sampling –≤–∞–∂–µ–Ω –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤",
        "üìö –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ",
    ]
    return np.random.choice(tips)

# ============ –£–¢–ò–õ–ò–¢–´ –î–õ–Ø –†–ê–ë–û–¢–´ –° –î–ê–ù–ù–´–ú–ò ============
def get_file_size_mb(file) -> float:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –ú–ë."""
    file.seek(0, 2)  # Seek to end
    size = file.tell()
    file.seek(0)  # Reset
    return size / (1024 * 1024)

def sanitize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """–û—á–∏—Å—Ç–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤ –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤."""
    df = df.copy()
    df.columns = [str(c).strip().replace('\n', ' ').replace('\r', ' ') for c in df.columns]
    return df

def remove_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ‚úÖ –£–õ–£–ß–®–ï–ù–û: –£–¥–∞–ª–∏—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Å—Ç–æ–ª–±—Ü—ã.
    - –ü–æ –∏–º–µ–Ω–∞–º
    - –ü–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
    """
    df = df.copy()
    
    # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∏–º–µ–Ω–∞–º
    df = df.loc[:, ~df.columns.duplicated(keep='first')]
    
    # –£–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ª–±—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–¥–µ–Ω—Ç–∏—á–Ω—ã –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º
    cols_to_check = list(df.columns)
    duplicates_to_drop = set()
    
    for i, col1 in enumerate(cols_to_check):
        if col1 in duplicates_to_drop:
            continue
        for col2 in cols_to_check[i+1:]:
            if col2 in duplicates_to_drop:
                continue
            try:
                if df[col1].equals(df[col2]):
                    duplicates_to_drop.add(col2)
                    logger.info(f"–°—Ç–æ–ª–±–µ—Ü '{col2}' –∏–¥–µ–Ω—Ç–∏—á–µ–Ω '{col1}', –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω")
            except Exception:
                pass
    
    if duplicates_to_drop:
        df = df.drop(columns=list(duplicates_to_drop), errors='ignore')
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Å—Ç–æ–ª–±—Ü–æ–≤: {len(duplicates_to_drop)}")
    
    return df

# ============ ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: MEMORY MANAGEMENT ============
def estimate_memory_usage(df: pd.DataFrame) -> float:
    """
    ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –û—Ü–µ–Ω–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ DataFrame –≤ –ú–ë.
    """
    memory_bytes = df.memory_usage(deep=True).sum()
    return memory_bytes / (1024 * 1024)

def optimize_dtypes(df: pd.DataFrame, aggressive: bool = False) -> pd.DataFrame:
    """
    ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.
    
    Args:
        df: DataFrame –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        aggressive: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é (–º–æ–∂–µ—Ç –ø–æ—Ç–µ—Ä—è—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å)
    
    Returns:
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
    """
    df_optimized = df.copy()
    
    for col in df_optimized.columns:
        col_type = df_optimized[col].dtype
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö —Ç–∏–ø–æ–≤
        if pd.api.types.is_integer_dtype(col_type):
            c_min = df_optimized[col].min()
            c_max = df_optimized[col].max()
            
            if c_min >= 0:
                if c_max < 255:
                    df_optimized[col] = df_optimized[col].astype(np.uint8)
                elif c_max < 65535:
                    df_optimized[col] = df_optimized[col].astype(np.uint16)
                elif c_max < 4294967295:
                    df_optimized[col] = df_optimized[col].astype(np.uint32)
            else:
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df_optimized[col] = df_optimized[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df_optimized[col] = df_optimized[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df_optimized[col] = df_optimized[col].astype(np.int32)
        
        elif pd.api.types.is_float_dtype(col_type) and aggressive:
            df_optimized[col] = df_optimized[col].astype(np.float32)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è object —Ç–∏–ø–æ–≤
        elif col_type == 'object':
            num_unique_values = df_optimized[col].nunique()
            num_total_values = len(df_optimized[col])
            
            if num_unique_values / num_total_values < 0.5:
                df_optimized[col] = df_optimized[col].astype('category')
    
    memory_before = estimate_memory_usage(df)
    memory_after = estimate_memory_usage(df_optimized)
    memory_saved = memory_before - memory_after
    
    if memory_saved > 0:
        logger.info(f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤: —ç–∫–æ–Ω–æ–º–∏—è {memory_saved:.1f} –ú–ë "
                   f"({memory_saved/memory_before*100:.1f}%)")
    
    return df_optimized
